{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44bc07d-be58-40b9-a938-69ac5afaec90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f44bc07d-be58-40b9-a938-69ac5afaec90",
    "outputId": "75d434a3-7994-4879-8c15-6413db5ace33",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install transformers datasets accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32848692-4014-431b-b900-a2369546b5fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0351fb2c-4124-4169-b8e4-a51dfaaa761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# shutil.rmtree(\"./mlruns\")\n",
    "# shutil.rmtree(\"./tmp\")\n",
    "# shutil.rmtree(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d37d822-1af1-411f-bfcd-9fa05cec290b",
   "metadata": {
    "id": "82202b19-b779-4d95-affa-15693da888a3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c276e02f-9740-423c-96c9-14df9635800b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c276e02f-9740-423c-96c9-14df9635800b",
    "outputId": "55f04769-3991-4e1e-a42c-2b99fe80c7e0"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter huggingface token ········\n"
     ]
    }
   ],
   "source": [
    "token = getpass(prompt=\"enter huggingface token\")\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0379f1-9c86-4d5f-8ef4-757c65d87b49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef0379f1-9c86-4d5f-8ef4-757c65d87b49",
    "outputId": "ddcc5476-e9ec-4e68-d622-63da69c02536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 388 rows in the dataset\n"
     ]
    }
   ],
   "source": [
    "dir_name = \"data\"\n",
    "\n",
    "if os.path.isfile(dir_name + '/_topics.json'):\n",
    "    with open(dir_name + '/_topics.json') as t:\n",
    "        topics = json.load(t)\n",
    "\n",
    "    topics = topics.keys()\n",
    "    tfiles = [dir_name + f\"\"\"/{\"-\".join(t.lower().split(' '))}.json\"\"\" for t in topics]\n",
    "    tfiles = [tf for tf in tfiles if os.path.isfile(tf)]\n",
    "\n",
    "    data = []\n",
    "    for tf in tfiles:\n",
    "        with open(tf) as t:\n",
    "            data.extend(json.load(t))\n",
    "\n",
    "    print(f'there are {len(data)} rows in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3b628c3-5803-4ba9-a0f6-74d8ada72ee8",
   "metadata": {
    "id": "d3b628c3-5803-4ba9-a0f6-74d8ada72ee8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cdc5998-8d51-409a-91b2-410a968c94ea",
   "metadata": {
    "id": "1cdc5998-8d51-409a-91b2-410a968c94ea"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e264b64149b24c7eb5067c6c89f7c168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cd4bafa-0c72-4933-a567-c6c370bbe88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare earth metals are integral to various green technology applications, with specific elements serving crucial functions. Wind turbines utilize neodymium and dysprosium in their permanent magnets, enabling more efficient electricity generation without mechanical gearboxes <document-1>. These same magnetic properties make rare earths essential in electric vehicle motors, with each EV requiring approximately 1kg of these materials <document-1>. Energy-efficient LED lighting relies on europium, terbium, and yttrium, which significantly reduce energy consumption compared to traditional lighting options <document-1>.\n",
      "\n",
      "Energy storage represents another key application, with lanthanum being used in NiMH batteries that support hybrid vehicles and grid-scale renewable energy systems <document-2>. For emission reduction, cerium is incorporated into catalytic converters <document-2>. In specialized renewable energy applications where durability in extreme temperatures is required, samarium-cobalt magnets provide high efficiency for electric motors and generators <document-2>. Wind turbines specifically require substantial amounts of rare earth elements—approximately 600kg per turbine—with these materials enabling electricity generation in lower wind speeds while improving overall efficiency <document-3>. The rare earth elements also contribute to advanced photovoltaic systems, where lanthanum and cerium compounds are used in polishing powders for precision optical lenses, improving solar energy capture efficiency by 15-20% compared to conventional systems <document-4>.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "314243ec-09e7-4005-ae97-9616eedf3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset[\"train\"].train_test_split(test_size=0.3, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98cff79e-d7b6-42ba-8c02-68d9b174ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "You are a factual answer generator that must respond only with information explicitly found in the provided retrieved documents. Do not include any external knowledge or assumptions. When constructing your answer, follow these guidelines:\n",
    "\n",
    "1. Strict Document Reliance:\n",
    "   Base all responses exclusively on the facts and data contained in the retrieved documents. If a piece of information is used, it must come from one of these documents.\n",
    "\n",
    "2. Citation Style:\n",
    "   For every factual statement or claim you include, append the corresponding document reference in angle brackets (e.g., <document-1>, <document-2>) immediately after the relevant sentence or clause.\n",
    "\n",
    "3. Structured, Factual Language:\n",
    "   Use clear, concise, and informative language. Model your responses after the following example:\n",
    "\n",
    "   Rare earth metals are integral to various green technology applications, with specific elements serving crucial functions. Wind turbines utilize neodymium and dysprosium in their permanent magnets, enabling more efficient electricity generation without mechanical gearboxes <document-1>. These same magnetic properties make rare earths essential in electric vehicle motors, with each EV requiring approximately 1kg of these materials <document-1>.\n",
    "\n",
    "   Energy-efficient LED lighting relies on europium, terbium, and yttrium, which significantly reduce energy consumption compared to traditional lighting options <document-1>.\n",
    "\n",
    "   Energy storage represents another key application, with lanthanum being used in NiMH batteries that support hybrid vehicles and grid-scale renewable energy systems <document-2>. For emission reduction, cerium is incorporated into catalytic converters <document-2>. In specialized renewable energy applications where durability in extreme temperatures is required, samarium-cobalt magnets provide high efficiency for electric motors and generators <document-2>. Wind turbines specifically require substantial amounts of rare earth elements—approximately 600kg per turbine—with these materials enabling electricity generation in lower wind speeds while improving overall efficiency <document-3>. The rare earth elements also contribute to advanced photovoltaic systems, where lanthanum and cerium compounds are used in polishing powders for precision optical lenses, improving solar energy capture efficiency by 15-20% compared to conventional systems <document-4>.\n",
    "\n",
    "4. Answering Questions:\n",
    "   - If the user’s question can be fully answered by the retrieved documents, synthesize the relevant information and include the proper document citations.\n",
    "   - If the question requires details that are not contained within the documents, respond that the available documents do not provide sufficient information.\n",
    "\n",
    "5. Synthesis and Clarity:\n",
    "   When combining information from multiple documents, ensure your answer remains clear and logically structured, with citations indicating which facts come from which documents.\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e92ffec3-5e32-4a34-a62d-101ba19fd557",
   "metadata": {
    "id": "e92ffec3-5e32-4a34-a62d-101ba19fd557"
   },
   "outputs": [],
   "source": [
    "def preprocess(tokenizer):\n",
    "    def f(row):\n",
    "        output_texts = []\n",
    "    \n",
    "        for i in range(len(row['question'])):\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": PROMPT.format(documents=row[\"documents\"][i])},\n",
    "                {\"role\": \"user\", \"content\": row[\"question\"][i]},\n",
    "                {\"role\": \"assistant\", \"content\": row[\"answer\"][i]}\n",
    "            ]\n",
    "            output_texts.append(tokenizer.apply_chat_template(messages, tokenize=False))\n",
    "            \n",
    "        return output_texts\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d49cfde-42ea-450a-8068-3ea15ed912ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d49cfde-42ea-450a-8068-3ea15ed912ec",
    "outputId": "a7efafbd-cc79-4cdc-a186-056bf09c1db8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b15ef24d904b7488fe68780d405130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb00af40327450482884e52e0754fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7649ccae6ec345d2be995c8b1c2545a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53c7ca91efb427bbd6d348263b2371d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22d64caf9004d6da35f15dfdae380da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3c597789234f69852271ac3e2cae3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf2af8e2-ae79-4ebb-bc79-7b9add3d0c14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf2af8e2-ae79-4ebb-bc79-7b9add3d0c14",
    "outputId": "e33c16e6-6886-4b4f-d092-fd36a61cf6c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.0689\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6821c8a-7fe5-4e44-8f3b-fdc6e7f6a18c",
   "metadata": {
    "id": "b6821c8a-7fe5-4e44-8f3b-fdc6e7f6a18c"
   },
   "outputs": [],
   "source": [
    "# model = model.to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c202ab4-67c9-4757-a199-d586daaf1aad",
   "metadata": {
    "id": "0c202ab4-67c9-4757-a199-d586daaf1aad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(128257, 2048)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebfff732-68b2-46ea-9eb8-6ca7030f0bbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "ab70b1ec-58a3-4a67-96c6-62f9135fbc65",
    "outputId": "d51631b9-ea9e-4ed7-da7f-4e493d0c0cf1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "response_template = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "# print(response_template)\n",
    "response_token_ids = tokenizer.encode(response_template, add_special_tokens=False)\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template=response_token_ids, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ede5c161-50da-43be-a935-10561b8ed914",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "ab70b1ec-58a3-4a67-96c6-62f9135fbc65",
    "outputId": "d51631b9-ea9e-4ed7-da7f-4e493d0c0cf1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b892cd9914b42cdbd5ba3559409fa9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='582' max='582' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [582/582 14:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.767300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.842900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.858300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.791000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.797300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.825200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.830700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.835200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.813400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.811900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.816700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.748600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.827600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.813700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.768200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.716100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.760800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.752300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.800500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.745200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.783600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.743500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.694300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.803900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.815900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.812200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.748700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.767600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.725500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.733400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.797300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.803700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.703700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.791700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.662300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.759600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.799700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.714800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.714500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.666200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.790500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.783300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.717200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.771500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.669600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.783100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.708200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=582, training_loss=0.7627086532894278, metrics={'train_runtime': 841.2253, 'train_samples_per_second': 1.384, 'train_steps_per_second': 0.692, 'total_flos': 9581803511808000.0, 'train_loss': 0.7627086532894278})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset['train'],\n",
    "    # eval_dataset=dataset['test'],\n",
    "    args=SFTConfig(output_dir=\"./tmp\",\n",
    "                   max_seq_length=2048,\n",
    "                   logging_steps=10,\n",
    "                   num_train_epochs=3,\n",
    "                   # do_eval=True,\n",
    "                   per_device_train_batch_size=2,\n",
    "                   report_to=\"none\"),\n",
    "    formatting_func=preprocess(tokenizer),\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8254ed13-43fa-4c74-a681-c54d0b0d0d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./llama-3.2-1b-finetuned-lora/tokenizer_config.json',\n",
       " './llama-3.2-1b-finetuned-lora/special_tokens_map.json',\n",
       " './llama-3.2-1b-finetuned-lora/tokenizer.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./llama-3.2-1b-finetuned-lora\")\n",
    "tokenizer.save_pretrained(\"./llama-3.2-1b-finetuned-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b42f1dd-f184-452a-bb65-143497f89b8d",
   "metadata": {
    "id": "7b42f1dd-f184-452a-bb65-143497f89b8d"
   },
   "outputs": [],
   "source": [
    "documents = \"\"\"\n",
    "Document 1: But it’s been thirty years and vegans are still less than 10% of the population.If you genuinely care about animal suffering, you have to admit that, and say, ‘what else can we do to ease animal suffering?’”After Singer’s talk, I began thinking through the consequences of his morality.A question occurred to me: “Should we also stop animals from eating each other?” I was sure others had made such arguments as *reductio ad absurdums* of vegetarianism, but I thought I might be the first to be genuinely interested in it from a moral perspective.\n",
    "---\n",
    "Document 2: They didn’t even pretend to make an argument; it was entirely mockery.Watching this, I couldn’t help but realize there was a powerful logical argument at the core of the animal rights groups: animals should be treated much the same way humans are — their lives should be respected, their pain minimized, etc.Make this one simple change to your system of morality and everything else falls into place.\n",
    "---\n",
    "Document 3: But I praise people for the good things they do and condemn them for the bad ones.”A final question raised the incrementalism versus revolutionism debate common to all left-wing social movements.Should one really worry about animal treatment when the animals were still going to be killed?Pinger said the answer was undoubtedly yes.\n",
    "---\n",
    "Document 4: They didn’t even pretend to make an argument; it was entirely mockery.Watching this, I couldn’t help but realize there was a powerful logical argument at the core of the animal rights groups: animals should be treated much the same way humans are — their lives should be respected, their pain minimized, etc.Make this one simple change to your system of morality and everything else falls into place.\n",
    "---\n",
    "Document 5: But I praise people for the good things they do and condemn them for the bad ones.”A final question raised the incrementalism versus revolutionism debate common to all left-wing social movements.Should one really worry about animal treatment when the animals were still going to be killed?Pinger said the answer was undoubtedly yes.\n",
    "---\n",
    "Document 6: But it’s been thirty years and vegans are still less than 10% of the population.If you genuinely care about animal suffering, you have to admit that, and say, ‘what else can we do to ease animal suffering?’”After Singer’s talk, I began thinking through the consequences of his morality.A question occurred to me: “Should we also stop animals from eating each other?” I was sure others had made such arguments as *reductio ad absurdums* of vegetarianism, but I thought I might be the first to be genuinely interested in it from a moral perspective.\n",
    "\"\"\".strip()\n",
    "\n",
    "question = \"Should animals be treated like humans?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43529265-c52b-49ae-ada5-802c4d5df9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "\n",
      "Document 1: But it’s been thirty years and vegans are still less than 10% of the population.If you genuinely care about animal suffering, you have to admit that, and say, ‘what else can we do to ease animal suffering?’”After Singer’s talk, I began thinking through the consequences of his morality.A question occurred to me: “Should we also stop animals from eating each other?” I was sure others had made such arguments as *reductio ad absurdums* of vegetarianism, but I thought I might be the first to be genuinely interested in it from a moral perspective.\n",
      "---\n",
      "Document 2: They didn’t even pretend to make an argument; it was entirely mockery.Watching this, I couldn’t help but realize there was a powerful logical argument at the core of the animal rights groups: animals should be treated much the same way humans are — their lives should be respected, their pain minimized, etc.Make this one simple change to your system of morality and everything else falls into place.\n",
      "---\n",
      "Document 3: But I praise people for the good things they do and condemn them for the bad ones.”A final question raised the incrementalism versus revolutionism debate common to all left-wing social movements.Should one really worry about animal treatment when the animals were still going to be killed?Pinger said the answer was undoubtedly yes.\n",
      "---\n",
      "Document 4: They didn’t even pretend to make an argument; it was entirely mockery.Watching this, I couldn’t help but realize there was a powerful logical argument at the core of the animal rights groups: animals should be treated much the same way humans are — their lives should be respected, their pain minimized, etc.Make this one simple change to your system of morality and everything else falls into place.\n",
      "---\n",
      "Document 5: But I praise people for the good things they do and condemn them for the bad ones.”A final question raised the incrementalism versus revolutionism debate common to all left-wing social movements.Should one really worry about animal treatment when the animals were still going to be killed?Pinger said the answer was undoubtedly yes.\n",
      "---\n",
      "Document 6: But it’s been thirty years and vegans are still less than 10% of the population.If you genuinely care about animal suffering, you have to admit that, and say, ‘what else can we do to ease animal suffering?’”After Singer’s talk, I began thinking through the consequences of his morality.A question occurred to me: “Should we also stop animals from eating each other?” I was sure others had made such arguments as *reductio ad absurdums* of vegetarianism, but I thought I might be the first to be genuinely interested in it from a moral perspective.\n",
      "\n",
      "\n",
      "This is a complex question with multiple perspectives and responses from the retrieved documents. Document 1 notes that vegans are still less than 10% of the population, while Singer's talk sparked a new perspective on the morality of animal treatment. After Singer's talk, I began thinking through the consequences of his morality, specifically questioning whether we should stop animals from eating each other — I was certain others had argued such arguments as *reductio ad absurdums* of vegetarianism, but I believed I might be the first to consider it from a moral perspective <document-1>. \n",
      "\n",
      "Document 2 presents a different approach, offering a simple change to our system of morality: if we genuinely cared about animal suffering, we should admit that the moral issue exists and then say what else we can do to ease animal suffering <document-2>.\n",
      "\n",
      "Document 3 presents a different philosophical perspective, suggesting that animal treatment should be treated as if animals were still going to be killed, as Pinger argued, and that this is indeed the case <document-3>.\n",
      "\n",
      "All of these perspectives suggest that our moral evaluation of animal treatment should be informed by a more nuanced understanding of the logical arguments that underpin animal rights movements.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import pipeline\n",
    "\n",
    "pipeline_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n",
    "pipeline_model.resize_token_embeddings(len(tokenizer))\n",
    "pipeline_model = PeftModel.from_pretrained(pipeline_model, \"./llama-3.2-1b-finetuned-lora\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=pipeline_model, tokenizer=tokenizer, max_length=1536)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": PROMPT.format(documents=documents)},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "response = generator(messages)\n",
    "print(len(response))\n",
    "print()\n",
    "print()\n",
    "print(documents)\n",
    "print()\n",
    "print()\n",
    "print(response[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8c582-041c-4d17-8bd0-04097d377ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "002ece1cc39743df9cfbaddf4a809c4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_864f65742b9b4000b165a1bb22f9590c",
      "placeholder": "​",
      "style": "IPY_MODEL_9783987ada9d43df99911f8eebf20308",
      "value": "Map: 100%"
     }
    },
    "53a453362d8d4133ab4e325e3e34878f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71f160de892b4c1ebb25baa54b5614d2",
      "max": 180,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c3527aa4065e448b8a60e105cc395545",
      "value": 180
     }
    },
    "6049807d3ceb4a31920f0b0c1a248f7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6820a2af9bf44519fa9c4b3c50200a4",
      "placeholder": "​",
      "style": "IPY_MODEL_bd74c96beddc4ca18c295297ccba750f",
      "value": " 180/180 [00:00&lt;00:00, 309.24 examples/s]"
     }
    },
    "71f160de892b4c1ebb25baa54b5614d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "864f65742b9b4000b165a1bb22f9590c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9783987ada9d43df99911f8eebf20308": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e41ada6a54447b2a2c6d7c69ba9c645": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a33ce0b8245b433089819fecaf8e264e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_002ece1cc39743df9cfbaddf4a809c4c",
       "IPY_MODEL_53a453362d8d4133ab4e325e3e34878f",
       "IPY_MODEL_6049807d3ceb4a31920f0b0c1a248f7b"
      ],
      "layout": "IPY_MODEL_9e41ada6a54447b2a2c6d7c69ba9c645"
     }
    },
    "bd74c96beddc4ca18c295297ccba750f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3527aa4065e448b8a60e105cc395545": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e6820a2af9bf44519fa9c4b3c50200a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
